{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.90</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.9240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.77</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.5523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex    bmi  children smoker     region     charges\n",
       "0   19  female  27.90         0    yes  southwest  16884.9240\n",
       "1   18    male  33.77         1     no  southeast   1725.5523"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the DataFrame\n",
    "df = pd.read_csv('insurance.csv')\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the X and Y variables\n",
    "X = df.drop('charges',axis = 1)\n",
    "Y = df[['charges']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Train Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "label_sex = LabelEncoder()\n",
    "label_smoke = LabelEncoder()\n",
    "onehot_region = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "X_train['sex'] = label_sex.fit_transform(X_train['sex']) # Label Encoding\n",
    "X_test['sex'] = label_sex.transform(X_test['sex'])\n",
    "\n",
    "X_train['smoker'] = label_smoke.fit_transform(X_train['smoker'])  # Label Encoding\n",
    "X_test['smoker'] = label_smoke.transform(X_test['smoker'])\n",
    "\n",
    "\n",
    "X_train_trans = onehot_region.fit_transform(X_train[['region']])  # One Hot Encoding\n",
    "X_test_trans = onehot_region.transform(X_test[['region']])\n",
    "col_names = onehot_region.get_feature_names_out()\n",
    "\n",
    "# Convert Dataframe and Reset index\n",
    "encoded_df_train = pd.DataFrame(X_train_trans, columns=col_names).reset_index(drop=True)\n",
    "encoded_df_test = pd.DataFrame(X_test_trans, columns=col_names).reset_index(drop=True)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the Region column\n",
    "X_train = X_train.drop('region',axis = 1)\n",
    "X_test = X_test.drop('region',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the one hot encoded columns\n",
    "X_train = pd.concat([X_train, encoded_df_train], axis=1)\n",
    "X_test = pd.concat([X_test, encoded_df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir/hyperparameter_tuning/tuner0.json\n",
      "Number of layers: 4\n",
      "Neurons in first hidden layer: 10\n",
      "Neurons in layer 2: 40\n",
      "Neurons in layer 3: 20\n",
      "Neurons in layer 4: 40\n",
      "Neurons in layer 5: 20\n",
      "Optimizer: adam\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/ANN-Regression/venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 12847.0498 - mae: 12847.0498 - val_loss: 14270.4590 - val_mae: 14270.4590\n",
      "Epoch 2/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13410.4189 - mae: 13410.4189 - val_loss: 14258.1357 - val_mae: 14258.1357\n",
      "Epoch 3/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13338.2061 - mae: 13338.2061 - val_loss: 14155.8965 - val_mae: 14155.8965\n",
      "Epoch 4/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12957.7617 - mae: 12957.7617 - val_loss: 13537.1582 - val_mae: 13537.1582\n",
      "Epoch 5/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12220.3730 - mae: 12220.3730 - val_loss: 11589.0244 - val_mae: 11589.0244\n",
      "Epoch 6/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9781.6494 - mae: 9781.6494 - val_loss: 9682.0410 - val_mae: 9682.0410\n",
      "Epoch 7/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8487.4141 - mae: 8487.4141 - val_loss: 8838.8164 - val_mae: 8838.8164\n",
      "Epoch 8/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7380.3560 - mae: 7380.3560 - val_loss: 8185.7007 - val_mae: 8185.7007\n",
      "Epoch 9/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6866.1958 - mae: 6866.1958 - val_loss: 7615.6392 - val_mae: 7615.6392\n",
      "Epoch 10/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6421.1113 - mae: 6421.1113 - val_loss: 7018.6372 - val_mae: 7018.6372\n",
      "Epoch 11/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5871.8335 - mae: 5871.8335 - val_loss: 6315.7480 - val_mae: 6315.7480\n",
      "Epoch 12/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5389.3550 - mae: 5389.3550 - val_loss: 5714.3354 - val_mae: 5714.3354\n",
      "Epoch 13/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4623.9087 - mae: 4623.9087 - val_loss: 5099.6201 - val_mae: 5099.6201\n",
      "Epoch 14/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4378.9380 - mae: 4378.9380 - val_loss: 4693.7446 - val_mae: 4693.7446\n",
      "Epoch 15/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4134.8423 - mae: 4134.8423 - val_loss: 4487.1162 - val_mae: 4487.1162\n",
      "Epoch 16/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3497.6782 - mae: 3497.6782 - val_loss: 4285.4990 - val_mae: 4285.4990\n",
      "Epoch 17/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3862.7639 - mae: 3862.7639 - val_loss: 4259.2588 - val_mae: 4259.2588\n",
      "Epoch 18/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3837.9915 - mae: 3837.9915 - val_loss: 4182.9307 - val_mae: 4182.9307\n",
      "Epoch 19/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3656.3396 - mae: 3656.3396 - val_loss: 4137.5000 - val_mae: 4137.5000\n",
      "Epoch 20/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3773.3225 - mae: 3773.3225 - val_loss: 4047.9253 - val_mae: 4047.9253\n",
      "Epoch 21/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3642.2710 - mae: 3642.2710 - val_loss: 4017.8652 - val_mae: 4017.8652\n",
      "Epoch 22/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3421.4094 - mae: 3421.4094 - val_loss: 3910.7681 - val_mae: 3910.7681\n",
      "Epoch 23/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3252.4216 - mae: 3252.4216 - val_loss: 3857.4570 - val_mae: 3857.4570\n",
      "Epoch 24/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3318.4182 - mae: 3318.4182 - val_loss: 3816.3047 - val_mae: 3816.3047\n",
      "Epoch 25/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3343.3733 - mae: 3343.3733 - val_loss: 3773.8101 - val_mae: 3773.8101\n",
      "Epoch 26/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3523.9944 - mae: 3523.9944 - val_loss: 3743.4597 - val_mae: 3743.4597\n",
      "Epoch 27/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3548.6035 - mae: 3548.6035 - val_loss: 3670.8760 - val_mae: 3670.8760\n",
      "Epoch 28/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3425.7415 - mae: 3425.7415 - val_loss: 3618.6792 - val_mae: 3618.6792\n",
      "Epoch 29/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3295.8096 - mae: 3295.8096 - val_loss: 3550.6418 - val_mae: 3550.6418\n",
      "Epoch 30/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3380.9097 - mae: 3380.9097 - val_loss: 3501.0811 - val_mae: 3501.0811\n",
      "Epoch 31/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3554.4929 - mae: 3554.4929 - val_loss: 3482.1311 - val_mae: 3482.1311\n",
      "Epoch 32/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3191.2690 - mae: 3191.2690 - val_loss: 3408.5493 - val_mae: 3408.5493\n",
      "Epoch 33/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3503.1853 - mae: 3503.1853 - val_loss: 3327.7654 - val_mae: 3327.7654\n",
      "Epoch 34/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3226.1675 - mae: 3226.1675 - val_loss: 3222.3669 - val_mae: 3222.3669\n",
      "Epoch 35/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2970.8232 - mae: 2970.8232 - val_loss: 3105.5544 - val_mae: 3105.5544\n",
      "Epoch 36/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2753.7471 - mae: 2753.7471 - val_loss: 3032.5359 - val_mae: 3032.5359\n",
      "Epoch 37/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3014.0884 - mae: 3014.0884 - val_loss: 2922.0173 - val_mae: 2922.0173\n",
      "Epoch 38/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2625.8503 - mae: 2625.8503 - val_loss: 2808.6794 - val_mae: 2808.6794\n",
      "Epoch 39/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2930.0596 - mae: 2930.0596 - val_loss: 2763.7507 - val_mae: 2763.7507\n",
      "Epoch 40/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2694.8103 - mae: 2694.8103 - val_loss: 2738.1589 - val_mae: 2738.1589\n",
      "Epoch 41/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2543.6006 - mae: 2543.6006 - val_loss: 2705.5737 - val_mae: 2705.5737\n",
      "Epoch 42/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2730.9758 - mae: 2730.9758 - val_loss: 2799.6284 - val_mae: 2799.6284\n",
      "Epoch 43/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2752.1582 - mae: 2752.1582 - val_loss: 2667.6411 - val_mae: 2667.6411\n",
      "Epoch 44/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2762.1306 - mae: 2762.1306 - val_loss: 2647.5796 - val_mae: 2647.5796\n",
      "Epoch 45/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2575.1729 - mae: 2575.1729 - val_loss: 2620.8169 - val_mae: 2620.8169\n",
      "Epoch 46/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2437.5356 - mae: 2437.5356 - val_loss: 2608.5581 - val_mae: 2608.5581\n",
      "Epoch 47/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2531.2163 - mae: 2531.2163 - val_loss: 2555.1006 - val_mae: 2555.1006\n",
      "Epoch 48/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2850.0808 - mae: 2850.0808 - val_loss: 2534.4109 - val_mae: 2534.4109\n",
      "Epoch 49/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2405.3909 - mae: 2405.3909 - val_loss: 2497.2703 - val_mae: 2497.2703\n",
      "Epoch 50/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2704.3796 - mae: 2704.3796 - val_loss: 2468.0557 - val_mae: 2468.0557\n",
      "Epoch 51/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2477.3301 - mae: 2477.3301 - val_loss: 2444.8462 - val_mae: 2444.8462\n",
      "Epoch 52/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2131.6353 - mae: 2131.6353 - val_loss: 2418.6077 - val_mae: 2418.6077\n",
      "Epoch 53/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2316.1299 - mae: 2316.1299 - val_loss: 2376.5400 - val_mae: 2376.5400\n",
      "Epoch 54/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2387.5562 - mae: 2387.5562 - val_loss: 2358.8606 - val_mae: 2358.8606\n",
      "Epoch 55/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2382.3054 - mae: 2382.3054 - val_loss: 2315.5930 - val_mae: 2315.5930\n",
      "Epoch 56/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2130.1914 - mae: 2130.1914 - val_loss: 2263.1836 - val_mae: 2263.1836\n",
      "Epoch 57/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2204.3196 - mae: 2204.3196 - val_loss: 2249.8386 - val_mae: 2249.8386\n",
      "Epoch 58/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2248.7791 - mae: 2248.7791 - val_loss: 2216.6016 - val_mae: 2216.6016\n",
      "Epoch 59/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2259.4534 - mae: 2259.4534 - val_loss: 2224.0786 - val_mae: 2224.0786\n",
      "Epoch 60/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1910.7406 - mae: 1910.7406 - val_loss: 2144.3042 - val_mae: 2144.3042\n",
      "Epoch 61/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2198.9341 - mae: 2198.9341 - val_loss: 2127.2427 - val_mae: 2127.2427\n",
      "Epoch 62/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2312.5437 - mae: 2312.5437 - val_loss: 2103.2241 - val_mae: 2103.2241\n",
      "Epoch 63/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2007.7469 - mae: 2007.7469 - val_loss: 2075.7131 - val_mae: 2075.7131\n",
      "Epoch 64/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1958.8533 - mae: 1958.8533 - val_loss: 2059.6787 - val_mae: 2059.6787\n",
      "Epoch 65/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1939.1240 - mae: 1939.1240 - val_loss: 2054.9294 - val_mae: 2054.9294\n",
      "Epoch 66/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1900.4010 - mae: 1900.4010 - val_loss: 2012.6709 - val_mae: 2012.6709\n",
      "Epoch 67/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1823.2771 - mae: 1823.2771 - val_loss: 2003.9314 - val_mae: 2003.9314\n",
      "Epoch 68/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2255.0850 - mae: 2255.0850 - val_loss: 1992.9570 - val_mae: 1992.9570\n",
      "Epoch 69/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2085.4268 - mae: 2085.4268 - val_loss: 2016.5133 - val_mae: 2016.5133\n",
      "Epoch 70/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1894.6958 - mae: 1894.6958 - val_loss: 1971.3269 - val_mae: 1971.3269\n",
      "Epoch 71/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2105.0369 - mae: 2105.0369 - val_loss: 1936.4257 - val_mae: 1936.4257\n",
      "Epoch 72/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1920.7371 - mae: 1920.7371 - val_loss: 1926.3076 - val_mae: 1926.3076\n",
      "Epoch 73/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1956.6788 - mae: 1956.6788 - val_loss: 1913.9589 - val_mae: 1913.9589\n",
      "Epoch 74/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2233.4441 - mae: 2233.4441 - val_loss: 1886.0847 - val_mae: 1886.0847\n",
      "Epoch 75/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2113.0896 - mae: 2113.0896 - val_loss: 1877.3929 - val_mae: 1877.3929\n",
      "Epoch 76/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1864.5227 - mae: 1864.5227 - val_loss: 1862.5848 - val_mae: 1862.5848\n",
      "Epoch 77/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1862.3687 - mae: 1862.3687 - val_loss: 1840.8627 - val_mae: 1840.8627\n",
      "Epoch 78/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1911.3611 - mae: 1911.3611 - val_loss: 1840.6344 - val_mae: 1840.6344\n",
      "Epoch 79/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1935.2245 - mae: 1935.2245 - val_loss: 1815.4700 - val_mae: 1815.4700\n",
      "Epoch 80/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1876.3184 - mae: 1876.3184 - val_loss: 1799.3364 - val_mae: 1799.3364\n",
      "Epoch 81/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1867.5948 - mae: 1867.5948 - val_loss: 1788.5746 - val_mae: 1788.5746\n",
      "Epoch 82/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1812.8167 - mae: 1812.8167 - val_loss: 1790.1395 - val_mae: 1790.1395\n",
      "Epoch 83/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1851.5339 - mae: 1851.5339 - val_loss: 1763.1016 - val_mae: 1763.1016\n",
      "Epoch 84/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1799.5206 - mae: 1799.5206 - val_loss: 1761.6056 - val_mae: 1761.6056\n",
      "Epoch 85/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2000.0968 - mae: 2000.0968 - val_loss: 1742.2196 - val_mae: 1742.2196\n",
      "Epoch 86/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1604.9010 - mae: 1604.9010 - val_loss: 1720.5547 - val_mae: 1720.5547\n",
      "Epoch 87/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1635.2826 - mae: 1635.2826 - val_loss: 1714.7568 - val_mae: 1714.7568\n",
      "Epoch 88/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1936.7939 - mae: 1936.7939 - val_loss: 1700.8027 - val_mae: 1700.8027\n",
      "Epoch 89/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1900.5522 - mae: 1900.5522 - val_loss: 1693.9883 - val_mae: 1693.9883\n",
      "Epoch 90/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1971.6046 - mae: 1971.6046 - val_loss: 1685.3539 - val_mae: 1685.3539\n",
      "Epoch 91/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1834.2080 - mae: 1834.2080 - val_loss: 1672.5435 - val_mae: 1672.5435\n",
      "Epoch 92/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1842.6395 - mae: 1842.6395 - val_loss: 1672.1033 - val_mae: 1672.1033\n",
      "Epoch 93/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1912.6499 - mae: 1912.6499 - val_loss: 1663.7728 - val_mae: 1663.7728\n",
      "Epoch 94/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2023.0487 - mae: 2023.0487 - val_loss: 1681.5641 - val_mae: 1681.5641\n",
      "Epoch 95/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1775.7354 - mae: 1775.7354 - val_loss: 1644.2421 - val_mae: 1644.2421\n",
      "Epoch 96/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1796.6411 - mae: 1796.6411 - val_loss: 1646.9331 - val_mae: 1646.9331\n",
      "Epoch 97/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1880.9257 - mae: 1880.9257 - val_loss: 1638.8494 - val_mae: 1638.8494\n",
      "Epoch 98/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1804.4342 - mae: 1804.4342 - val_loss: 1624.4987 - val_mae: 1624.4987\n",
      "Epoch 99/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1761.6368 - mae: 1761.6368 - val_loss: 1618.7920 - val_mae: 1618.7920\n",
      "Epoch 100/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1956.6053 - mae: 1956.6053 - val_loss: 1613.8354 - val_mae: 1613.8354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7a5de6b92160>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter tuning, model building, training\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adding the first hidden layer   \n",
    "    model.add(Dense(units=hp.Int('units1', min_value=10, max_value=100, step=10),\n",
    "                    activation='relu', input_dim=X_train.shape[1]))\n",
    "                    \n",
    "    # Adding consecutive hidden layers and deciding the number of neurons              \n",
    "    for i in range(hp.Int('num_layers', 1, 10)):  \n",
    "        model.add(Dense(units=hp.Int(f'units_{i+2}', min_value=10, max_value=100, step=10),\n",
    "                        activation='relu')) \n",
    "\n",
    "    # Defining the output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Selecting the optimizer\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup Keras Tuner with Hyperband algorithm\n",
    "tuner = kt.Hyperband(\n",
    "build_model,\n",
    "objective='val_loss',\n",
    "max_epochs=10,  \n",
    "factor=3,  \n",
    "directory='my_dir',  \n",
    "project_name='hyperparameter_tuning')\n",
    "\n",
    "# Run hyperparameter search\n",
    "tuner.search(X_train, Y_train, epochs=10, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Get the best hyperparameters found by the tuner\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the best model based on found hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Number of layers: {best_hps['num_layers']}\")\n",
    "print(f\"Neurons in first hidden layer: {best_hps['units1']}\")\n",
    "for i in range(best_hps['num_layers']):\n",
    "    print(f\"Neurons in layer {i+2}: {best_hps[f'units_{i+2}']}\")\n",
    "print(f\"Optimizer: {best_hps['optimizer']}\")\n",
    "\n",
    "\n",
    "# Adding early_stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  # Monitor the validation loss\n",
    "                               patience=5,  # Number of epochs to wait for improvement\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "# Train the best model\n",
    "best_model.fit(X_train, Y_train, epochs = 100, validation_data=(X_test, Y_test), callbacks=early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1594.9526 - mae: 1594.9526\n",
      "MAE: 1613.83544921875\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model\n",
    "test_loss, test_mae = best_model.evaluate(X_test, Y_test)\n",
    "print(f\"MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Saving the model in h5 format\n",
    "best_model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
